{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of RecSys M Ex3 TEMPLATE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4JwV4EsDm75"
      },
      "source": [
        "2576183S"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THQFNe3zdt1f"
      },
      "source": [
        "# RecSys M 2021 - Exercise 3 Template\n",
        "\n",
        "The aims of this exercise are:\n",
        " - Explore a different recommendation dataset\n",
        " - Develop and evaluate baseline recommender systems\n",
        " - Implement hybrid recommender models\n",
        " - Explore diversification issues in recommender systems\n",
        " - Revise other material from the lectures.\n",
        "\n",
        "As usual, there is a corresponding Quiz on Moodle for this Exercise, which should be answered as you proceed. For more details, see the Exercise 3 specification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvTdMyXdODex"
      },
      "source": [
        "# Part-Pre. Preparation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww--_kl9-ndn"
      },
      "source": [
        "## Pre 1. Setup Block\n",
        "\n",
        "This exercise will use the [Goodreads]() dataset for books. These blocks setup the data files, Python etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFgYpbhh0tkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b5e157-9168-4c35-f87a-287075f6ef81"
      },
      "source": [
        "!rm -rf ratings* books* to_read* test*\n",
        "\n",
        "!curl -o ratings.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-ratings.csv\" \n",
        "!curl -o books.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-books.csv\"\n",
        "!curl -o to_read.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-to_read.csv\"\n",
        "!curl -o test.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-test.csv\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 7631k  100 7631k    0     0  4805k      0  0:00:01  0:00:01 --:--:-- 4802k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2366k  100 2366k    0     0  2558k      0 --:--:-- --:--:-- --:--:-- 2555k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 7581k  100 7581k    0     0  7705k      0 --:--:-- --:--:-- --:--:-- 7697k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1895k  100 1895k    0     0  2366k      0 --:--:-- --:--:-- --:--:-- 2363k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VpVnNrZ1EiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2e8a06-10a2-4033-de4d-7bf3bd084147"
      },
      "source": [
        "#Standard setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "!pip install git+https://github.com/cmacdonald/spotlight.git@master#egg=spotlight\n",
        "from spotlight.interactions import Interactions\n",
        "SEED=20\n",
        "BPRMF=None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spotlight\n",
            "  Cloning https://github.com/cmacdonald/spotlight.git (to revision master) to /tmp/pip-install-4b464kh1/spotlight_50c58705fdef4109b1e4609b00b2a424\n",
            "  Running command git clone -q https://github.com/cmacdonald/spotlight.git /tmp/pip-install-4b464kh1/spotlight_50c58705fdef4109b1e4609b00b2a424\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spotlight) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->spotlight) (3.7.4.3)\n",
            "Building wheels for collected packages: spotlight\n",
            "  Building wheel for spotlight (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spotlight: filename=spotlight-0.1.6-py3-none-any.whl size=34106 sha256=c7827ba95467f8f451abc8ff41574f7345b8622b5f38c37f45750a74b4b24554\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-olsq7jq7/wheels/1c/2a/31/d187173520bc800643df4e3d1f97dee21d2133ba41085704ed\n",
            "Successfully built spotlight\n",
            "Installing collected packages: spotlight\n",
            "Successfully installed spotlight-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtJO0e0m-hun"
      },
      "source": [
        "## Pre 2. Data Preparation\n",
        "\n",
        "Let's load the dataset into dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKAb25iw1MYw"
      },
      "source": [
        "#load in the csv files\n",
        "ratings_df = pd.read_csv(\"ratings.csv\")\n",
        "books_df = pd.read_csv(\"books.csv\")\n",
        "to_read_df = pd.read_csv(\"to_read.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6rqfn53OhDC"
      },
      "source": [
        "#cut down the number of items and users\n",
        "counts=ratings_df[ratings_df[\"book_id\"] < 2000].groupby([\"book_id\"]).count().reset_index()\n",
        "valid_books=counts[counts[\"user_id\"] >= 10][[\"book_id\"]]\n",
        "\n",
        "books_df = books_df.merge(valid_books, on=\"book_id\")\n",
        "ratings_df = ratings_df[ratings_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
        "to_read_df = to_read_df[to_read_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
        "test = test[test[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
        "\n",
        "\n",
        "#stringify the id columns\n",
        "def str_col(df):\n",
        "  if \"user_id\" in df.columns:\n",
        "    df[\"user_id\"] = \"u\" + df.user_id.astype(str)\n",
        "  if \"book_id\" in df.columns:\n",
        "    df[\"book_id\"] = \"b\" + df.book_id.astype(str)\n",
        "\n",
        "str_col(books_df)\n",
        "str_col(ratings_df)\n",
        "str_col(to_read_df)\n",
        "str_col(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7cgXhmYUXIn"
      },
      "source": [
        "Here we construct the Interactions objects from `ratings.csv`, `to_read.csv` and `test.csv`. We manually specify the num_users and num_items parameters to all Interactions objects, in case the test set differs from your training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ClgJOdTTt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7361b3af-bcf9-44c3-cb97-af4b11655786"
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "iid_map = defaultdict(count().__next__)\n",
        "\n",
        "\n",
        "rating_iids = np.array([iid_map[iid] for iid in ratings_df[\"book_id\"].values], dtype = np.int32)\n",
        "test_iids = np.array([iid_map[iid] for iid in test[\"book_id\"].values], dtype = np.int32)\n",
        "toread_iids = np.array([iid_map[iid] for iid in to_read_df[\"book_id\"].values], dtype = np.int32)\n",
        "\n",
        "\n",
        "uid_map = defaultdict(count().__next__)\n",
        "test_uids = np.array([uid_map[uid] for uid in test[\"user_id\"].values], dtype = np.int32)\n",
        "rating_uids = np.array([uid_map[uid] for uid in ratings_df[\"user_id\"].values], dtype = np.int32)\n",
        "toread_uids = np.array([uid_map[iid] for iid in to_read_df[\"user_id\"].values], dtype = np.int32)\n",
        "\n",
        "\n",
        "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
        "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
        "\n",
        "\n",
        "rating_dataset = Interactions(user_ids=rating_uids,\n",
        "                               item_ids=rating_iids,\n",
        "                               ratings=ratings_df[\"rating\"].values,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "toread_dataset = Interactions(user_ids=toread_uids,\n",
        "                               item_ids=toread_iids,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "test_dataset = Interactions(user_ids=test_uids,\n",
        "                               item_ids=test_iids,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "print(rating_dataset)\n",
        "print(toread_dataset)\n",
        "print(test_dataset)\n",
        "\n",
        "#here we define the validation set\n",
        "toread_dataset_train, validation = random_train_test_split(toread_dataset, random_state=np.random.RandomState(SEED))\n",
        "\n",
        "num_items = test_dataset.num_items\n",
        "num_users = test_dataset.num_users"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Interactions dataset (1999 users x 1826 items x 124762 interactions)>\n",
            "<Interactions dataset (1999 users x 1826 items x 135615 interactions)>\n",
            "<Interactions dataset (1999 users x 1826 items x 33917 interactions)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2noK30pBEsF"
      },
      "source": [
        "Finally, this is some utility code that we will use in the exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kDxZgICBFp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50165bc8-d567-4568-a648-149b3b77fcb9"
      },
      "source": [
        "def getAuthorTitle(iid):\n",
        "  bookid = iid_rev_map[iid]\n",
        "  row = books_df[books_df.book_id == bookid]\n",
        "  return row.iloc[0][\"authors\"] + \" / \" + row.iloc[0][\"title\"]\n",
        "\n",
        "print(\"iid 0: \" + getAuthorTitle(0) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iid 0: Carlos Ruiz ZafÃ³n, Lucia Graves / The Shadow of the Wind (The Cemetery of Forgotten Books,  #1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt4I2C5DTUL5"
      },
      "source": [
        "## Pre 3. Example Code\n",
        "\n",
        "To evaluate some of your hand-implemented recommender systems (e.g. Q1, Q4), you will need to instantiate objects that match the specification of a Spotlight model, which `mrr_score()` etc. expects.\n",
        "\n",
        "\n",
        "Here is an example recommender object that returns 0 for each item, regardless of user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eaxy_hakbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a6b516-603b-4d50-a417-6b6efec7d2b9"
      },
      "source": [
        "from spotlight.evaluation import mrr_score, precision_recall_score\n",
        "\n",
        "class dummymodel:\n",
        "  \n",
        "  def __init__(self, numitems):\n",
        "    self.predictions=np.zeros(numitems)\n",
        "  \n",
        "  #uid is the user we are requesting recommendations for;\n",
        "  #returns an array of scores, one for each item\n",
        "  def predict(self, uid):\n",
        "    #this model returns all zeros, regardless of userid\n",
        "    return( self.predictions )\n",
        "\n",
        "#lets evaluate how the effeciveness of dummymodel\n",
        "\n",
        "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100).mean())\n",
        "#as expected, a recommendation model that gives 0 scores for all items obtains a MRR score of 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQTJOmS5dB3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ddac10-83e3-4a92-c276-688a7f9bd513"
      },
      "source": [
        "#note that mrr_score() displays a progress bar if you set verbose=True\n",
        "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100, verbose=True).mean())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1999it [00:00, 2821.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCWXwVC5Mtyj"
      },
      "source": [
        "# Part-A. Combination of Recommendation Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyvGgW_3ZjLV"
      },
      "source": [
        "## Task 1. Explicit & Implicit Matrix Factorisation Models\n",
        "\n",
        "Create and train three matrix factorisation systems:\n",
        " - \"EMF\": explicit MF, trained on the ratings Interactions object (`rating_dataset`)\n",
        " - \"IMF\": implicit MF, trained on the toread_dataset Interactions object (`toread_dataset_train`)\n",
        " - \"BPRMF\": implicit MF with the BPR loss function (`loss='bpr'`), trained on the toread_dataset Interactions object (`toread_dataset_train`)\n",
        "\n",
        "Use a variable of the same name for these models, as we will use some of them later (e.g. `BPRMF`).\n",
        "  \n",
        "In all cases, you must use the standard initialisation arguments, i.e. \n",
        "`n_iter=10, embedding_dim=32, use_cuda=False, random_state=np.random.RandomState(SEED)`.\n",
        " \n",
        "Evaluate each of these models in terms of Mean Reciprocal Rank on the test set. MRR can be obtained using:\n",
        "```python\n",
        "mrr_score(X, test_dataset, train=rating_dataset, k=100, verbose=True).mean())\n",
        "```\n",
        "where X is an instance of a Spotlight model. Do NOT change the `k` or `train` arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDADjtepRvpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1228294d-1ea3-4d91-85dc-4ed3f4d44a26"
      },
      "source": [
        "# Add your solution here\n",
        "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
        "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
        "import time  \n",
        "\n",
        "\n",
        "#a)\n",
        "EMF = ExplicitFactorizationModel(n_iter=10,\n",
        "                                    embedding_dim=32, \n",
        "                                    use_cuda=False,\n",
        "                                    random_state=np.random.RandomState(SEED) \n",
        ")\n",
        "current = time.time()\n",
        "EMF.fit(rating_dataset, verbose=True)\n",
        "end = time.time()\n",
        "\n",
        "print(\"EMF Training took %d seconds \\n \"% (end - current))\n",
        "\n",
        "mrr_EMF = mrr_score(EMF, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
        "print(\"MRR score for EMF : %f\" %(mrr_EMF.mean()))\n",
        "\n",
        "print(\"\\n------------------------------------------------\")\n",
        "\n",
        "#b)\n",
        "IMF = ImplicitFactorizationModel( n_iter=10, \n",
        "                                    embedding_dim=32, \n",
        "                                    use_cuda=False,\n",
        "                                    random_state=np.random.RandomState(SEED)\n",
        ")\n",
        "current = time.time()\n",
        "IMF.fit(toread_dataset_train, verbose=True)\n",
        "end = time.time()\n",
        "\n",
        "print(\"IMF Training took %d seconds \\n\" % (end - current))\n",
        "\n",
        "mrr_IMF = mrr_score(IMF, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
        "print(\"MRR score for IMF : %f\" %(mrr_IMF.mean()))\n",
        "\n",
        "print(\"\\n------------------------------------------------\")\n",
        "\n",
        "#c)\n",
        "BPRMF = ImplicitFactorizationModel( n_iter=10, \n",
        "                                    embedding_dim=32, \n",
        "                                    use_cuda=False,\n",
        "                                    random_state=np.random.RandomState(SEED), \n",
        "                                    loss='bpr'\n",
        ")\n",
        "current = time.time()\n",
        "BPRMF.fit(toread_dataset_train, verbose=True)\n",
        "end = time.time()\n",
        "\n",
        "print(\"BPRMF Training took %d seconds. \\n\" % (end - current))\n",
        "\n",
        "mrr_BPRMF = mrr_score(BPRMF, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
        "print(\"MRR score for BPRMF : %f\" %(mrr_BPRMF.mean()))\n",
        "\n",
        "print(\"\\n------------------------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: loss 3.8710271667261593\n",
            "Epoch 1: loss 0.7940810446123607\n",
            "Epoch 2: loss 0.6382512643200452\n",
            "Epoch 3: loss 0.5217335281557725\n",
            "Epoch 4: loss 0.44844855655167926\n",
            "Epoch 5: loss 0.40543351120880394\n",
            "Epoch 6: loss 0.3823863151254224\n",
            "Epoch 7: loss 0.36336620252762664\n",
            "Epoch 8: loss 0.35137936695799477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "104it [00:00, 1032.10it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9: loss 0.3396692546237199\n",
            "EMF Training took 11 seconds \n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "203it [00:00, 1018.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "309it [00:00, 1029.82it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "407it [00:00, 1013.17it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "515it [00:00, 1029.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "620it [00:00, 1034.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "728it [00:00, 1046.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "837it [00:00, 1059.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "938it [00:00, 1036.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1038it [00:01, 1016.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1137it [00:01, 982.89it/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1236it [00:01, 984.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1342it [00:01, 1005.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1445it [00:01, 1011.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1550it [00:01, 1020.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1659it [00:01, 1037.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1766it [00:01, 1043.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1871it [00:01, 1035.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1999it [00:01, 1018.58it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MRR score for EMF : 0.058984\n",
            "\n",
            "------------------------------------------------\n",
            "Epoch 0: loss 0.7677980539090229\n",
            "Epoch 1: loss 0.53877861825925\n",
            "Epoch 2: loss 0.47017199658560305\n",
            "Epoch 3: loss 0.428322009882837\n",
            "Epoch 4: loss 0.39839018825090156\n",
            "Epoch 5: loss 0.368275504770144\n",
            "Epoch 6: loss 0.3473479778699155\n",
            "Epoch 7: loss 0.32980164804689166\n",
            "Epoch 8: loss 0.31870100696413023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "120it [00:00, 1194.03it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9: loss 0.3048194432103971\n",
            "IMF Training took 17 seconds \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "217it [00:00, 1116.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "320it [00:00, 1088.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "419it [00:00, 1056.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "511it [00:00, 1010.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "601it [00:00, 973.15it/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "704it [00:00, 989.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "812it [00:00, 1013.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "908it [00:00, 996.36it/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1004it [00:01, 975.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1099it [00:01, 965.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1206it [00:01, 992.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1305it [00:01, 985.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1415it [00:01, 1015.66it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1522it [00:01, 1028.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1625it [00:01, 1013.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1732it [00:01, 1028.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1838it [00:01, 1035.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1999it [00:01, 1008.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MRR score for IMF : 0.329932\n",
            "\n",
            "------------------------------------------------\n",
            "Epoch 0: loss 0.33895447579616644\n",
            "Epoch 1: loss 0.19644999289709442\n",
            "Epoch 2: loss 0.15870640168563938\n",
            "Epoch 3: loss 0.14147728193059284\n",
            "Epoch 4: loss 0.132827276100387\n",
            "Epoch 5: loss 0.12213623321632731\n",
            "Epoch 6: loss 0.11668406535853755\n",
            "Epoch 7: loss 0.11047121562626001\n",
            "Epoch 8: loss 0.10888675406996934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "107it [00:00, 1057.60it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9: loss 0.10400472129782978\n",
            "BPRMF Training took 17 seconds. \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "204it [00:00, 1029.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "315it [00:00, 1050.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "423it [00:00, 1058.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "528it [00:00, 1053.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "630it [00:00, 1042.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "729it [00:00, 1021.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "823it [00:00, 980.32it/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "920it [00:00, 975.31it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1027it [00:01, 999.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1130it [00:01, 1005.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1235it [00:01, 1013.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1338it [00:01, 1018.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1448it [00:01, 1041.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1552it [00:01, 1035.44it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1656it [00:01, 1033.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1762it [00:01, 1039.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1867it [00:01, 1041.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1999it [00:01, 1024.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MRR score for BPRMF : 0.407677\n",
            "\n",
            "------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZHCOmfEDOGo"
      },
      "source": [
        "## Task 2. Hybrid Model\n",
        "\n",
        "In this task, you are expected to create new hybrid recommendation models that \n",
        "combine the two models in Task 1, namely IMF and BPRMF. \n",
        "\n",
        "(a) Linearly combine the *scores* from IMF and BPRMF.  Normalise both input scores into the range 0..1 using [sklearn's minmax_scale() function](\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) before combining them.\n",
        "\n",
        "(b) Apply a pipelining recommender, where the top 100 items are obtained from IMF and re-ranked using the scores of BPRMF. Items not returned by IMF get a score of 0.\n",
        "\n",
        "To implement these hybrid models, you should create new classes that abide by the Spotlight model contract (namely, it has a `predict(self, uid)` function that returns a score for *all* items). \n",
        "\n",
        "Evaluate each model in terms of MRR. How many users are improved, how many are degraded compared to the BPRMF baseline?\n",
        "\n",
        "Finally, pass your instantiated model object to the `test_Hybrid_a()` (for (a)) or `test_Hybrid_b()` (for (b)) functions, as appropriate, and record the results in the quiz. For example, if your model for (b) is called `pipeline`, then you would run:\n",
        "```python\n",
        "test_Hybrid_b(pipeline)\n",
        "```\n",
        "\n",
        "You now have sufficient information to answer the Task 2 quiz questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MnRziixSGQq"
      },
      "source": [
        "# len(IMF.predict(0))\n",
        "# b = (BPRMF.predict(0))\n",
        "\n",
        "# a = IMF.predict(0)\n",
        "# a_in = np.argsort(a)[::-1]\n",
        "# a[23]\n",
        "# # a[1204](min)\n",
        "# (np.argmin(a))\n",
        "# # a[23]\n",
        "# # a[1204]\n",
        "# x = a_in[0:100]\n",
        "# for i in x:\n",
        "#   print(b[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j6JzIOHkYw9"
      },
      "source": [
        "def test_Hybrid_a(combsumObj):\n",
        "  for i, u in enumerate([5, 20]):\n",
        "    print(\"Hybrid a test case %d\" % i)\n",
        "    print(np.count_nonzero(combsumObj.predict(u) > 1))\n",
        "\n",
        "def test_Hybrid_b(pipeObj):\n",
        "  for i, iid in enumerate([3, 0]):\n",
        "    print(\"Hybrid b test case %d\" % i)\n",
        "    print(pipeObj.predict(0)[iid])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_o7a1ppFZ7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5f754d-3f2a-4860-baa6-369fdc27e66f"
      },
      "source": [
        "# Add your solutions here and evaluate them\n",
        "from sklearn import preprocessing\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "\n",
        "class linearModel:\n",
        "  \n",
        "  def __init__(self, num_items):\n",
        "    self.predictions = np.zeros(num_items)\n",
        "  \n",
        "  def predict(self, uid):\n",
        "\n",
        "    imf_score = IMF.predict(uid)\n",
        "    bprmf_score = BPRMF.predict(uid)\n",
        "    norm_IMF_score = preprocessing.minmax_scale(imf_score, feature_range = (0,1))\n",
        "    norm_BPRMF_score = preprocessing.minmax_scale(bprmf_score, feature_range = (0,1))\n",
        "\n",
        "    #combsumobj = Scores of sums from each input ranking\n",
        "    val = norm_IMF_score + norm_BPRMF_score\n",
        "\n",
        "    return val\n",
        "\n",
        "linear_model_mrr = mrr_score(linearModel(num_items), test_dataset, train=rating_dataset, k=100)\n",
        "print(\"Linear Model MRR score :\", linear_model_mrr.mean())\n",
        "\n",
        "deg = 0\n",
        "imp = 0\n",
        "\n",
        "for i in range(0,len(mrr_BPRMF)):\n",
        "  if mrr_BPRMF[i] < linear_model_mrr[i]:\n",
        "    imp += 1\n",
        "  elif mrr_BPRMF[i] > linear_model_mrr[i]:\n",
        "    deg +=1\n",
        "\n",
        "print('RR improved by linear :', imp)\n",
        "print('RR degraded by linear :', deg)\n",
        "\n",
        "\n",
        "class pipeModel:\n",
        "  \n",
        "  def __init__(self, num_items):\n",
        "    self.predictions = np.zeros(num_items)\n",
        "  \n",
        "  def predict(self, uid):\n",
        "\n",
        "    imf_score = IMF.predict(uid)\n",
        "    bprmf_score = BPRMF.predict(uid)\n",
        "    imf_score_sort = np.argsort(imf_score)[::-1][0:100]\n",
        "    \n",
        "    imf_reranked = np.zeros(len(bprmf_score))\n",
        "\n",
        "    for i in imf_score_sort:\n",
        "      imf_reranked[i] = bprmf_score[i]\n",
        "\n",
        "    return imf_reranked\n",
        "\n",
        "pipeline_model_mrr = mrr_score(pipeModel(num_items), test_dataset, train=rating_dataset, k=100)\n",
        "print(\"Pipeline Model MRR score :\", pipeline_model_mrr.mean())\n",
        "\n",
        "deg = 0\n",
        "imp = 0\n",
        "\n",
        "for i in range(0,len(mrr_BPRMF)):\n",
        "  if mrr_BPRMF[i] < pipeline_model_mrr[i]:\n",
        "    imp += 1\n",
        "  elif mrr_BPRMF[i] > pipeline_model_mrr[i]:\n",
        "    deg +=1\n",
        "\n",
        "print('RR improved by pipeline :', imp)\n",
        "print('RR degraded by pipeline :', deg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Model MRR score : 0.41010939818893816\n",
            "RR improved by linear : 736\n",
            "RR degraded by linear : 745\n",
            "Pipeline Model MRR score : 0.41511955311399246\n",
            "RR improved by pipeline : 586\n",
            "RR degraded by pipeline : 214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROCBxlfKKkGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806c16d8-ecd3-44d8-c2f9-6bb1b14791ac"
      },
      "source": [
        "#Now test your hybrid approaches for the quiz\n",
        "\n",
        "test_Hybrid_a(linearModel(0))\n",
        "test_Hybrid_b(pipeModel(0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hybrid a test case 0\n",
            "445\n",
            "Hybrid a test case 1\n",
            "407\n",
            "Hybrid b test case 0\n",
            "22.013418197631836\n",
            "Hybrid b test case 1\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf0K6GECM0LQ"
      },
      "source": [
        "# Part-B. Analysing Recommendation Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gszqk2kIZLX"
      },
      "source": [
        "## Utility methods\n",
        "\n",
        "Below, we provide a function, `get_top_K(model, uid : int, k : int)` which, when provided with a Spotlight model, will provide the top k predictions for the specified uid. The iids, their scores, and their embeddings are returned. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIyc_p_dIm1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed0d944-ce5e-4de3-f44c-59f5d7fd07f9"
      },
      "source": [
        "from typing import Sequence, Tuple\n",
        "\n",
        "def get_top_K(model, uid : int, k : int) -> Tuple[ Sequence[int], Sequence[float],  np.ndarray ] :\n",
        "  #returns iids, their (normalised) scores in descending order, and item emebddings for the top k predictions of the given uid.\n",
        "\n",
        "  from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "  from scipy.stats import rankdata\n",
        "  # get scores from model\n",
        "  scores = model.predict(uid)\n",
        "\n",
        "  # map scores into rank 0..1 over the entire item space\n",
        "  scores = minmax_scale(scores)\n",
        "\n",
        "  #compute their ranks  \n",
        "  ranks = rankdata(-scores)\n",
        "  \n",
        "  # get and filter iids, scores and embeddings\n",
        "  rtr_scores = scores[ranks <= k]\n",
        "  rtr_iids = np.argwhere(ranks <= k).flatten()\n",
        "  if hasattr(model, '_net'):\n",
        "    embs = model._net.item_embeddings.weight[rtr_iids]\n",
        "  else:\n",
        "    # not a model that has any embeddings\n",
        "    embs = np.zeros([k,1])\n",
        "  \n",
        "  # identify correct ordering using numpy.argsort()\n",
        "  ordering = (-1*rtr_scores).argsort()\n",
        "  \n",
        "  #return iids, scores and their embeddings in descending order of score\n",
        "  return rtr_iids[ordering], rtr_scores[ordering], embs[ordering]\n",
        "\n",
        "if BPRMF is not None:\n",
        "  iids, scores, embs = get_top_K(BPRMF, 0, 10)\n",
        "  print(\"Returned iids: %s\" % str(iids))\n",
        "  print(\"Returned scores: %s\" % str(scores))\n",
        "  print(\"Returned embeddings: %s\" % str(embs))\n",
        "else:\n",
        "  print(\"You need to define BPRMF in Task 1\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Returned iids: [ 23 108  21  33   9  81  52 254  16   3]\n",
            "Returned scores: [1.         0.9895131  0.9848315  0.92250896 0.9070817  0.90654314\n",
            " 0.9005319  0.89310133 0.88378096 0.8836929 ]\n",
            "Returned embeddings: tensor([[-0.0453,  1.3716, -0.8307, -1.2616,  1.6700,  1.0161,  1.1168,  2.3530,\n",
            "         -1.2027,  0.8522, -1.0941, -0.6865, -0.5725, -2.0335, -1.2591,  0.6154,\n",
            "         -0.1374, -1.6868, -1.8615, -0.7514,  1.9909, -0.3909,  1.9239,  1.3293,\n",
            "         -1.2834, -0.4520,  1.1338,  0.3467,  2.5169, -2.1587,  1.2310,  1.1670],\n",
            "        [ 0.1239,  1.1004,  0.0531, -1.1045,  1.9932,  1.5049,  1.0011,  1.9734,\n",
            "         -1.6322, -0.8913, -0.6372,  0.7721, -1.1422, -2.2424, -1.1936, -0.5770,\n",
            "          0.0762, -1.0283, -1.2807, -2.0889,  2.8154, -0.9600, -0.1419,  0.8408,\n",
            "         -1.6067, -1.2905,  1.9169,  1.3988,  1.8646, -2.2028,  0.5365,  0.2022],\n",
            "        [ 0.3845,  0.8188, -0.1892, -1.1793,  2.1731,  0.6669,  1.1271,  1.4538,\n",
            "         -1.2173, -0.5447, -1.6713,  0.5249, -0.6132, -3.1082, -0.6489,  0.4312,\n",
            "          0.9176, -1.0346, -1.7232, -1.3347,  2.5504,  0.2789,  1.9649,  0.7684,\n",
            "         -1.0310, -1.3983,  0.8985, -0.0562,  2.1894, -0.8905,  1.0992,  0.6691],\n",
            "        [-0.4410,  0.4801, -0.2538, -0.5986,  1.2272,  0.6531,  1.4534,  1.3803,\n",
            "         -1.3796,  0.8305, -1.1837, -0.3366, -0.3528, -1.9982, -1.2018,  0.8934,\n",
            "         -0.5632, -0.6443, -0.7337, -0.4922,  2.9899,  0.2760,  1.4479,  1.0105,\n",
            "         -0.7107, -1.7105, -0.9456, -0.2314,  2.2862, -1.0982,  0.6176,  1.9784],\n",
            "        [-0.5686,  1.3279,  0.0929, -1.1565,  0.5140, -0.1223,  0.8788,  2.0444,\n",
            "          0.2803,  0.6417, -0.3809,  0.2828, -0.3895, -2.7013, -1.4182,  0.2742,\n",
            "         -1.0461, -1.5824, -2.0993, -1.3979,  1.3412, -0.4346,  1.5427,  1.2284,\n",
            "         -2.0168, -1.3083,  0.2939,  2.2792,  1.2569, -0.8994,  1.0784, -0.0203],\n",
            "        [ 0.1340,  0.2584, -0.5791, -0.5029,  2.7529,  0.0107,  0.8058,  2.3262,\n",
            "         -1.9017, -0.4165, -1.4422, -0.7689, -0.7657, -1.3836,  0.7729, -0.0596,\n",
            "          0.1377, -0.9144, -1.0304, -2.4873,  2.4421, -0.2146,  1.3113,  2.0114,\n",
            "         -0.5655, -1.5423,  1.9430,  2.1211,  1.2265, -0.4562,  0.4370,  1.1741],\n",
            "        [ 0.5933,  1.3073,  0.5726, -0.0917,  1.6623,  1.3124,  0.8131,  1.4753,\n",
            "         -1.6077,  1.4744, -0.6149, -0.1318,  0.2843, -2.1551, -1.0225,  1.1611,\n",
            "         -0.7732, -1.3496, -0.7587, -1.4566,  1.8771,  0.2448,  0.9532,  0.2902,\n",
            "         -1.4033, -1.9129,  1.0374, -0.1574,  2.0627, -1.1652,  0.8938,  0.7756],\n",
            "        [ 0.6018,  1.0445, -0.5415,  0.5355,  1.4569,  0.5330,  0.2956,  1.5574,\n",
            "         -0.2669, -1.4242,  1.5775,  1.0870, -0.6438, -1.5680, -1.4657,  1.3033,\n",
            "         -0.6602, -0.7102, -1.1306, -1.5143,  1.2747,  0.5494, -0.2278,  1.8629,\n",
            "         -1.8720, -0.3860,  1.0929,  1.4837,  1.2602, -1.6316, -0.4450,  0.6793],\n",
            "        [-0.4931, -0.2156, -1.0300, -1.1251,  2.3141,  0.1844,  0.0278,  1.1525,\n",
            "         -0.3218, -0.2236, -0.9952,  0.4091, -0.8533, -2.1377, -2.0955,  0.4107,\n",
            "         -0.5804, -1.6455, -1.4729, -2.6273,  1.5917, -0.3359,  2.3430,  0.6596,\n",
            "         -1.4888, -1.8436,  1.2947,  2.4997,  1.9382, -0.2631,  0.8980,  0.6717],\n",
            "        [-0.6251,  1.0291, -0.9705, -0.5551,  1.3933,  1.4241,  0.6316,  0.8137,\n",
            "         -0.1443, -0.4631,  0.1315, -0.3589, -0.3534, -1.7652, -0.1728,  0.4081,\n",
            "         -2.4594, -2.0278, -0.9450, -1.9469,  0.8574, -0.0176,  0.7410,  0.8268,\n",
            "         -0.9872, -1.0237,  1.6763,  1.2756,  1.2099, -0.8075,  1.2227,  1.8007]],\n",
            "       grad_fn=<IndexBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVYiQRYaEh61"
      },
      "source": [
        "## Task 3. Evaluation of Non-personalised Models\n",
        "Implement the following four (non-personalised) baselines for ranking books based on their statistics:\n",
        " - Average rating, obtained from ratings_df, `ratings` column\n",
        " - Number of ratings, obtained from books_df (column `ratings_count`)\n",
        " - Number of 5* ratings, obtained from books_df (column `ratings_5`)\n",
        " - Fraction of 5* ratings, calculated from the two sources of evidence above, i.e (columns  `ratings_5` and `ratings_count`).\n",
        "\n",
        "Evaluate these in terms of MRR using the provided test data. You may use the StaticModel class below. \n",
        "\n",
        "Hints: \n",
        " - As in Exercise 2, the order of items returned by predict() is _critical_. You may wish to refer to iid_map.\n",
        " - For all models, you need to ensure that your values are not cast to ints. If you are extracting values from a Pandas series, it is advised to use [.astype(np.float32)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xop8aPfyFucw"
      },
      "source": [
        "class StaticModel:\n",
        "  \n",
        "  def __init__(self, staticscores):\n",
        "    self.numitems = len(staticscores)\n",
        "    #print(self.numitems)\n",
        "    assert isinstance(staticscores, np.ndarray), \"Expected a numpy array\"\n",
        "    assert staticscores.dtype == np.float32 or staticscores.dtype == np.float64, \"Expected a numpy array of floats\"\n",
        "    self.staticscores = staticscores\n",
        "  \n",
        "  def predict(self, uid):\n",
        "    #this model returns the same scores for each user    \n",
        "    return self.staticscores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWmWPpik2fxu"
      },
      "source": [
        "#len(ratings_df['book_id'].unique())\n",
        "# ratings_df['rating'].min()\n",
        "# ratings_df['rating'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EAwbePE4JMa"
      },
      "source": [
        "# books_df.head(2)\n",
        "# books_df['ratings_count'].min()\n",
        "# books_df['ratings_count'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkM4qfnO4bY9"
      },
      "source": [
        "# iid_map.get('b123')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzbjSw70DXcZ"
      },
      "source": [
        "# books_df\n",
        "# books_df['book_id'][1]\n",
        "# for i,r in books_df.iterrows():\n",
        "#books_df.dtypes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R1q-Zm7FVM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6440b05d-b3ed-4986-983c-8234f7a56e99"
      },
      "source": [
        "# Add your solution here\n",
        "\n",
        "#a)\n",
        "a = np.zeros(num_items)\n",
        "for i, row in ratings_df.groupby(by=['book_id']).mean().iterrows():\n",
        "  a[iid_map[i]] = row['rating']\n",
        "\n",
        "model_a = StaticModel(a.astype(np.float32))\n",
        "mrr_a = mrr_score(model_a, test_dataset,train = rating_dataset, k=100, verbose=True)\n",
        "print('\\n MRR Score for model a(average rating) = ', mrr_a.mean())\n",
        "\n",
        "#b)\n",
        "temp1 = np.zeros(num_items)\n",
        "for i, row in books_df.iterrows():\n",
        "  temp1[iid_map[books_df['book_id'][i]]] = row['ratings_count']\n",
        "b = ( temp1 / np.max(temp1) ) * 5\n",
        "model_b = StaticModel(b.astype(np.float32))\n",
        "\n",
        "mrr_b = mrr_score(model_b, test_dataset,train = rating_dataset , k=100, verbose=True)\n",
        "print('\\n MRR Score for model b(number of ratings) = ', mrr_b.mean())\n",
        "\n",
        "#c)\n",
        "temp2 = np.zeros(num_items)\n",
        "for i, row in books_df.iterrows():\n",
        "  temp2[iid_map[books_df['book_id'][i]]] = row['ratings_5']\n",
        "c = ( temp2 / np.max(temp2) ) * 5\n",
        "model_c = StaticModel(c.astype(np.float32))\n",
        "\n",
        "mrr_c = mrr_score(model_c, test_dataset,train = rating_dataset , k=100, verbose=True)\n",
        "print('\\n MRR Score for model c(number of 5 rating) = ', mrr_c.mean())\n",
        "\n",
        "#d)\n",
        "temp3 = np.zeros(num_items)\n",
        "for i, row in books_df.iterrows(): \n",
        "  temp3[iid_map[books_df['book_id'][i]]] = (row['ratings_5'] / row['ratings_count'])\n",
        "d = ( temp3 / np.max(temp3) ) * 5\n",
        "model_d = StaticModel(d.astype(np.float32))\n",
        "\n",
        "mrr_d = mrr_score(model_d, test_dataset,train = rating_dataset , k=100, verbose=True)\n",
        "print('\\n MRR Score for model d(fraction of 5 rating) = ', mrr_d.mean())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "215it [00:00, 2148.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "421it [00:00, 2119.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "601it [00:00, 2009.49it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "790it [00:00, 1972.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "992it [00:00, 1983.50it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1201it [00:00, 2012.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1391it [00:00, 1975.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1591it [00:00, 1980.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1790it [00:00, 1982.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1999it [00:01, 1951.95it/s]\n",
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " MRR Score for model a(average rating) =  0.015052024168984034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "198it [00:00, 1979.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "419it [00:00, 2042.43it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "614it [00:00, 2011.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "823it [00:00, 2034.20it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1028it [00:00, 2037.87it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1242it [00:00, 2065.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1465it [00:00, 2110.22it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1687it [00:00, 2139.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1999it [00:00, 2090.20it/s]\n",
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " MRR Score for model b(number of ratings) =  0.2396001188245477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "221it [00:00, 2204.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "426it [00:00, 2154.23it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "645it [00:00, 2163.76it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "817it [00:00, 2005.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1019it [00:00, 2009.13it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1230it [00:00, 2036.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1438it [00:00, 2047.75it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1628it [00:00, 1998.61it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1999it [00:00, 2025.29it/s]\n",
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " MRR Score for model c(number of 5 rating) =  0.2409670879930144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "221it [00:00, 2204.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "420it [00:00, 2135.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "639it [00:00, 2149.67it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "831it [00:00, 2074.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1039it [00:00, 2074.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1255it [00:00, 2098.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1451it [00:00, 2052.65it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1652it [00:00, 2038.30it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1999it [00:00, 2039.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " MRR Score for model d(fraction of 5 rating) =  0.03415267465103555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5klEtfw0Ijyo"
      },
      "source": [
        "#d.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZFDmFcdXPFl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZqRSixpGvfn"
      },
      "source": [
        "## Task 4. Qualiatively Examining Recommendations\n",
        "\n",
        "From now on, we will consider the `BPRMF` model.\n",
        "\n",
        "In Recommender Systems, the ground truth (i.e. our list of books that the user has added to their \"to_read\" shelf) can be very incomplete. For instance, this can be because the user is not aware of the book yet.\n",
        "\n",
        "For this reason, it is important to \"eyeball\" the recommendations, to understand what the system is surfacing, and whether the recommendations make sense. In this way, we understand if the recommendations are reasonable, even if they are for books that the user has not actually read according to the test dataset.\n",
        "\n",
        "First, write a function, which given a uid (int), prints the *title and authors* of:\n",
        " - (a) the books that the user has previously shelved (c.f. `toread_dataset`)\n",
        " - (b) the books that the user will read in the future (c.f. `test_dataset`)\n",
        " - (c) the top 10 books that the user were recommended by `BPRMF` - you can make use of `get_top_K()`.\n",
        "\n",
        "You can use the previously defined `getAuthorTitle()` function in your solution.\n",
        "You will also want to compare books in (c) with those in (a) and (b).\n",
        "\n",
        "Then, we will examine two specific users, namely uid 1805 (u336) and uid 179 (user u1331), to analyse if their recommendations make sense. Refer to the Task 4 quiz questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd6o_LbwMw7_",
        "outputId": "8c034b15-5372-4ae3-e2b3-e8b40ad42c88"
      },
      "source": [
        "toread_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Interactions dataset (1999 users x 1826 items x 135615 interactions)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFO0g9XJNVWg"
      },
      "source": [
        "\n",
        "# for i in toread_dataset.item_ids[toread_dataset.user_ids == 0]:\n",
        "#   print(i)\n",
        "# getAuthorTitle(56)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg1eFa5GYv5c"
      },
      "source": [
        "# Add your solution here\n",
        "recommended = []\n",
        "future_read = []\n",
        "previously_shelved = []\n",
        "\n",
        "def get_title_author_4(uid):\n",
        "  print('\\nUID :', uid)\n",
        "  \n",
        "  \n",
        "  print('The books that the user has previously shelved : ')\n",
        "  for iid in toread_dataset.item_ids[toread_dataset.user_ids == uid]:\n",
        "    authorTitle = getAuthorTitle(iid)\n",
        "    previously_shelved.append(authorTitle)\n",
        "    print(iid,':',authorTitle)\n",
        "\n",
        "  print('(Totat count = %d)'%(len(previously_shelved)))\n",
        "  # print(previously_shelved)\n",
        "\n",
        "  \n",
        "  print('\\n The books that the user will read in the future: ')\n",
        "  for iid in test_dataset.item_ids[test_dataset.user_ids == uid]:\n",
        "    authorTitle = getAuthorTitle(iid)\n",
        "    future_read.append(authorTitle)\n",
        "    print(iid,':',authorTitle)\n",
        "  print('(Totat count = %d)'%(len(future_read)))\n",
        "  #print(future_read)\n",
        "\n",
        "  \n",
        "  k = 10\n",
        "  print('\\n The top 10 books that the user were recommended:') \n",
        "  recommended_iids, scores, embds = get_top_K(BPRMF, uid , k )\n",
        "  for iid in recommended_iids:\n",
        "    authorTitle = getAuthorTitle(iid)\n",
        "    recommended.append(authorTitle)\n",
        "    print(iid,':',authorTitle)\n",
        "  print(' (Totat count = %d)'%(len(recommended)))\n",
        "  # print(recommended)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MBewZ-qZIcH",
        "outputId": "6cff7248-9a61-405c-f450-739a37a07288"
      },
      "source": [
        "recommended = []\n",
        "future_read = []\n",
        "previously_shelved = []\n",
        "get_title_author_4(1805)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UID : 1805\n",
            "The books that the user has previously shelved : \n",
            "1348 : Stieg Larsson, Reg Keeland / The Girl Who Kicked the Hornet's Nest (Millennium, #3)\n",
            "1390 : Suzanne Collins / Mockingjay (The Hunger Games, #3)\n",
            "727 : Dennis Lehane / Shutter Island\n",
            "1342 : Suzanne Collins / Catching Fire (The Hunger Games, #2)\n",
            "1771 : Paula Hawkins / The Girl on the Train\n",
            "743 : Robert Ludlum / The Bourne Supremacy (Jason Bourne, #2)\n",
            "92 : John Grisham / The Client\n",
            "566 : Thomas Harris / The Silence of the Lambs  (Hannibal Lecter, #2)\n",
            "243 : Daphne du Maurier, Sally Beauman / Rebecca\n",
            "40 : Robert Ludlum / The Bourne Identity (Jason Bourne, #1)\n",
            "1687 : Robert Galbraith, J.K. Rowling / The Cuckoo's Calling (Cormoran Strike, #1)\n",
            "613 : Stephen King / Misery\n",
            "440 : Michael Crichton / Jurassic Park (Jurassic Park, #1)\n",
            "812 : Robert Ludlum / The Bourne Ultimatum (Jason Bourne, #3)\n",
            "167 : Stephen King, Bernie Wrightson / The Stand\n",
            "457 : Michael Crichton / The Andromeda Strain\n",
            "188 : Thomas Harris / Red Dragon (Hannibal Lecter, #1)\n",
            "1151 : Lee Child / Die Trying (Jack Reacher, #2)\n",
            "1444 : Lee Child / Worth Dying For (Jack Reacher, #15)\n",
            "1135 : Lee Child / Tripwire  (Jack Reacher, #3)\n",
            "19 : Michael Crichton / Congo\n",
            "1136 : Lee Child, Dick Hill / Without Fail (Jack Reacher, #6)\n",
            "18 : Michael Crichton / The Lost World (Jurassic Park, #2)\n",
            "615 : Janet Evanovich / One for the Money (Stephanie Plum, #1)\n",
            "535 : Tom Clancy / Patriot Games (Jack Ryan Universe, #2)\n",
            "1255 : Lee Child / Running Blind (Jack Reacher, #4)\n",
            "1050 : Ken Follett / Eye of the Needle\n",
            "47 : Michael Crichton / State of Fear\n",
            "1078 : Scott Turow / Presumed Innocent\n",
            "569 : Harlan Coben / Tell No One\n",
            "(Totat count = 30)\n",
            "\n",
            " The books that the user will read in the future: \n",
            "391 : John Grisham / The Pelican Brief\n",
            "1286 : Stieg Larsson, Reg Keeland / The Girl Who Played with Fire (Millennium, #2)\n",
            "1587 : Gillian Flynn / Gone Girl\n",
            "59 : Tom Clancy / The Hunt for Red October (Jack Ryan Universe, #4)\n",
            "369 : Chuck Palahniuk / Fight Club\n",
            "201 : Umberto Eco, William Weaver, SeÃ¡n Barrett / The Name of the Rose\n",
            "934 : John Grisham / The Runaway Jury\n",
            "850 : Thomas Harris / Hannibal (Hannibal Lecter, #3)\n",
            "1521 : Lee Child / The Affair (Jack Reacher, #16)\n",
            "51 : John Grisham / The Firm (Penguin Readers, Level 5)\n",
            "1251 : Lee Child / Killing Floor (Jack Reacher, #1)\n",
            "50 : John Grisham / A Time to Kill\n",
            "725 : Stephen King / The Shining (The Shining #1)\n",
            "42 : Michael Crichton / Timeline\n",
            "41 : Michael Crichton / Prey\n",
            "943 : Jeffery Deaver / The Bone Collector (Lincoln Rhyme, #1)\n",
            "(Totat count = 16)\n",
            "\n",
            " The top 10 books that the user were recommended:\n",
            "1270 : Suzanne Collins / The Hunger Games (The Hunger Games, #1)\n",
            "16 : Dan Brown / The Da Vinci Code (Robert Langdon, #2)\n",
            "1312 : Dan Brown / The Lost Symbol (Robert Langdon, #3)\n",
            "880 : Michael Crichton / Disclosure\n",
            "228 : George R.R. Martin / A Clash of Kings  (A Song of Ice and Fire, #2)\n",
            "75 : Dan Brown / Angels & Demons  (Robert Langdon, #1)\n",
            "86 : John Grisham / The Broker\n",
            "81 : Khaled Hosseini / The Kite Runner\n",
            "225 : George R.R. Martin / A Game of Thrones (A Song of Ice and Fire, #1)\n",
            "1390 : Suzanne Collins / Mockingjay (The Hunger Games, #3)\n",
            " (Totat count = 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2MiI27vW0wP",
        "outputId": "8b2463f9-cbd8-4f90-ff4a-16473887dff3"
      },
      "source": [
        "round(mrr_BPRMF[1805],4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fLUeeMIYzqy",
        "outputId": "e0b5874a-38ae-4650-d9de-62f4089005f2"
      },
      "source": [
        "a = [x for x in previously_shelved if x in recommended]\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Suzanne Collins / Mockingjay (The Hunger Games, #3)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0oqlm_DO-94",
        "outputId": "43f327ea-6a4b-4dff-a99e-0fdb4a080d93"
      },
      "source": [
        "a = [x for x in future_read if x in recommended]\n",
        "print(len(a))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAgKa0_oTxW8",
        "outputId": "fd37ba9b-fed2-4ab2-a98f-1a03048a0df7"
      },
      "source": [
        "get_title_author_4(179)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UID : 179\n",
            "The books that the user has previously shelved : \n",
            "75 : Dan Brown / Angels & Demons  (Robert Langdon, #1)\n",
            "1270 : Suzanne Collins / The Hunger Games (The Hunger Games, #1)\n",
            "244 : Antoine de Saint-ExupÃ©ry, Richard Howard, Dom Marcos Barbosa, Melina Karakosta / The Little Prince\n",
            "95 : Truman Capote / Breakfast at Tiffany's\n",
            "16 : Dan Brown / The Da Vinci Code (Robert Langdon, #2)\n",
            "926 : Laura Ingalls Wilder, Garth Williams / Little House on the Prairie (Little House, #2)\n",
            "147 : Milan Kundera, Michael Henry Heim / The Unbearable Lightness of Being\n",
            "1342 : Suzanne Collins / Catching Fire (The Hunger Games, #2)\n",
            "92 : John Grisham / The Client\n",
            "261 : J.R.R. Tolkien / The Lord of the Rings (The Lord of the Rings, #1-3)\n",
            "343 : J.R.R. Tolkien / The Hobbit\n",
            "180 : Margaret Mitchell / Gone with the Wind\n",
            "594 : Neil Gaiman / Stardust\n",
            "1021 : Laura Ingalls Wilder, Garth Williams / Little House in the Big Woods (Little House, #1)\n",
            "655 : Pearl S. Buck / The Good Earth (House of Earth, #1)\n",
            "778 : Dan Brown / Digital Fortress\n",
            "612 : Daniel Keyes / Flowers for Algernon\n",
            "365 : Neil Gaiman / Coraline\n",
            "453 : Dan Brown / Deception Point\n",
            "86 : John Grisham / The Broker\n",
            "906 : John Grisham / The Brethren\n",
            "249 : Agatha Christie / Murder on the Orient Express (Hercule Poirot, #10)\n",
            "91 : John Grisham / The King of Torts\n",
            "334 : Flora Rheta Schreiber / Sybil: The Classic True Story of a Woman Possessed by Sixteen Personalities\n",
            "88 : John Grisham / The Street Lawyer\n",
            "89 : John Grisham / The Partner\n",
            "103 : Anthony Bourdain / Kitchen Confidential: Adventures in the Culinary Underbelly\n",
            "62 : John Grisham / The Rainmaker\n",
            "557 : Astrid Lindgren, Lauren Child, Florence Lamborn, Nancy Seligsohn / Pippi Longstocking\n",
            "467 : Sidney Sheldon / If Tomorrow Comes (Tracy Whitney Series, #1)\n",
            "(Totat count = 60)\n",
            "\n",
            " The books that the user will read in the future: \n",
            "1390 : Suzanne Collins / Mockingjay (The Hunger Games, #3)\n",
            "581 : J.K. Rowling, Mary GrandPrÃ© / Harry Potter and the Deathly Hallows (Harry Potter, #7)\n",
            "1023 : Agatha Christie, Î¡Î¿Î¶Î¯ÏÎ± Î£ÏÎºÎ¿Ï / The Mysterious Affair at Styles (Hercule Poirot, #1)\n",
            "61 : John Grisham / The Testament\n",
            "87 : John Grisham / The Innocent Man: Murder and Injustice in a Small Town\n",
            "740 : John Grisham / A Painted House\n",
            "1189 : Agatha Christie / The Man in the Brown Suit\n",
            "136 : John Grisham / The Summons\n",
            "1128 : Paulo Coelho / The Witch Of Portobello\n",
            "1079 : Laura Ingalls Wilder, Garth Williams / Little Town on the Prairie  (Little House, #7)\n",
            "(Totat count = 26)\n",
            "\n",
            " The top 10 books that the user were recommended:\n",
            "89 : John Grisham / The Partner\n",
            "391 : John Grisham / The Pelican Brief\n",
            "92 : John Grisham / The Client\n",
            "906 : John Grisham / The Brethren\n",
            "88 : John Grisham / The Street Lawyer\n",
            "86 : John Grisham / The Broker\n",
            "62 : John Grisham / The Rainmaker\n",
            "91 : John Grisham / The King of Torts\n",
            "9 : J.K. Rowling, Mary GrandPrÃ© / Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\n",
            "934 : John Grisham / The Runaway Jury\n",
            " (Totat count = 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES_zHeCkNBeC"
      },
      "source": [
        "# Part-C. Diversity of Recommendations\n",
        "\n",
        "This part of the exercise is concerned with diversification, as covered in Lecture 11."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvBep-ROHWSX"
      },
      "source": [
        "## Task 5. Measuring Intra-List Diversity\n",
        "\n",
        "\n",
        "For the BPR implicit factorisation model, implement the Intra-list diversity measure (see Lecture 11) of the top 5 scored items based on their item embeddings in the `BPRMF` model. \n",
        "\n",
        "Implement your ILD as a function with the specification:\n",
        "```python\n",
        "def measure_ild(top_books : Sequence[int], K : int=5) -> float\n",
        "```\n",
        "where:\n",
        " - `top_books` is a list or a Numpy array of iids that have been returned for a particular user. For instance, it can be obtained from `get_top_K()`.\n",
        " - `K` is the number of top-ranked items to consider from `top_books`. \n",
        " - Your implementation should use the item emebddings stored in the `BPRMF` model.\n",
        "\n",
        "Calculate the ILD (with k=5). Using your code for Task 4, identify the books previously shelved and recommended for the specific users requested in the quiz, and use these to analyse the recommendations.\n",
        "\n",
        "Hints:\n",
        " - As can be seen in `get_top_K()`, item embeddings can be obtained from `BPRMF._net.item_embeddings.weight[iid]`.\n",
        " - For obtaining the cosine similarity of PyTorch tensors, use `nn.functional.cosine_similarity(, , axis=0)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgcVYc287VpL"
      },
      "source": [
        "# import numpy as np\n",
        "# arr = np.array([1,2,3,4,5])\n",
        "# cosine_similarities = []\n",
        "# l = len(arr)\n",
        "# d = []\n",
        "# for i in range(0,l):\n",
        "#   for j in range(i+1,l):\n",
        "#      d.append(1-(arr[i] - arr[j]))\n",
        "# print(np.sum(d)) \n",
        "# fin = 2 /(l * (l-1))   \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1pBa2C_-5od"
      },
      "source": [
        "# uids,scores,embd = get_top_K(BPRMF,0,5)\n",
        "# embd\n",
        "# # BPRMF._net.item_embeddings.weight[23]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AJBOy5pg4qd"
      },
      "source": [
        "# for i in range(0, 5):\n",
        "#     for j in range(0, 5):\n",
        "#       print(i,j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeJIIOEigjSH"
      },
      "source": [
        "# first = BPRMF._net.item_embeddings.weight[1]\n",
        "# second = BPRMF._net.item_embeddings.weight[0]\n",
        "# nn.functional.cosine_similarity(first ,second , axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n2vBwcnYuM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be48123-9dfd-48dc-ed14-1e68a69ad4c4"
      },
      "source": [
        "# Add your solution here\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def measure_ild(top_books : Sequence[int], K : int=5) -> float:\n",
        "  ILD = 0.0\n",
        "  iids_len = len(top_books)\n",
        "  d = []\n",
        "\n",
        "  for i in range(0, K):\n",
        "    for j in range(i+1, K):\n",
        "      if(i == j):\n",
        "        continue\n",
        "      first = BPRMF._net.item_embeddings.weight[top_books[i]]\n",
        "      second = BPRMF._net.item_embeddings.weight[top_books[j]]\n",
        "      cosine_similarities = nn.functional.cosine_similarity(first ,second , axis=0)\n",
        "      d.append(1 - cosine_similarities)\n",
        "  \n",
        "  ILD = (2 * np.sum(d)/ ( iids_len * (iids_len - 1))) \n",
        "  ILD = ILD.double().item()\n",
        "  return ILD\n",
        "\n",
        "iids,scores,embd = get_top_K(BPRMF , 0, 5)\n",
        "ILD = measure_ild(iids)\n",
        "print(ILD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20721983909606934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wz_2S8gOodG",
        "outputId": "464b05d7-12c6-4d2d-92cd-d30db6f87c28"
      },
      "source": [
        "iids,scores,embd = get_top_K(BPRMF , 1805, 5)\n",
        "ILD = measure_ild(iids)\n",
        "print(ILD)\n",
        "for i in iids:\n",
        "  print(getAuthorTitle(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7484899163246155\n",
            "Suzanne Collins / The Hunger Games (The Hunger Games, #1)\n",
            "Dan Brown / The Da Vinci Code (Robert Langdon, #2)\n",
            "Dan Brown / The Lost Symbol (Robert Langdon, #3)\n",
            "Michael Crichton / Disclosure\n",
            "George R.R. Martin / A Clash of Kings  (A Song of Ice and Fire, #2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtZhGd6CPMBL",
        "outputId": "db175bad-e7f6-45d8-85c1-2bba141dd1e1"
      },
      "source": [
        "iids,scores,embd = get_top_K(BPRMF , 179, 5)\n",
        "ILD = measure_ild(iids)\n",
        "print(round(ILD,4))\n",
        "for i in iids:\n",
        "  print(getAuthorTitle(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2787\n",
            "John Grisham / The Partner\n",
            "John Grisham / The Pelican Brief\n",
            "John Grisham / The Client\n",
            "John Grisham / The Brethren\n",
            "John Grisham / The Street Lawyer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qwrP1jUpARF"
      },
      "source": [
        "## Task 6. Implement MMR Diversification \n",
        "\n",
        "Develop an Maximal Marginal Relevance (M**M**R) diversification technique, to re-rank the top-ranked recommendations for a given user.\n",
        "\n",
        "Your function should adhere to the specification as follows:\n",
        "```python\n",
        "def mmr(iids : Sequence[int], scores : Sequence[float], embs : np.ndarray, alpha : float) -> Sequence[int]:\n",
        "```\n",
        "\n",
        "where iids is a list of iids, scores are their corresponding scores (in descending order), embs is their embeddings, and alpha controls the diversification tradeoff. The function returns a re-ordering of iids. As in previous Exercises, type hints are provided for clarity; a Sequence can be a list or numpy array. \n",
        "\n",
        "Hints:\n",
        " - As above, for obtaining the cosine similarity of PyTorch tensors, use nn.functional.cosine_similarity(, , axis=0).\n",
        "\n",
        "To use your `mmr()` function, provide it with the outputs of `get_top_K()`. For example, to obtain an MMR reordering of the top 10 predictions of uid 0, we can run:\n",
        "```\n",
        "mmr( *get_top_K(bprmodel, 0, 10), 0.5)\n",
        "```\n",
        "\n",
        "Thereafter, we provide test cases for your MMR implementation, which you  should report in the quiz. We also ask for the ILD values before and after the application of MMR.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzcTQxfImcSk"
      },
      "source": [
        "main_list = np.setdiff1d(R,S)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7cvRs9Zwjj_"
      },
      "source": [
        "# sim = np.array([[1,0.11,0.23,0.76,0.25],[0,1,0.29,0.57,0.51],[0,0,1,0.04,0.2,],[0,0,0,1,0.33],[0,0,0,0,1]])\n",
        "# scores = [0.91,0.9,0.5,0.06,0.63]\n",
        "# R = [0,1,2,3,4]\n",
        "# lam1 = 0.5\n",
        "# lam2 = 1 - lam1\n",
        "# s = []\n",
        "# final_scores =[]\n",
        "# x = 0\n",
        "\n",
        "# while len(R) > 0:\n",
        "   \n",
        "#     print('\\n iteration:', x )\n",
        "#     mrr_score = 0\n",
        "#     iid = ''\n",
        "#     # score_list = [0]\n",
        "#     for i in R:\n",
        "#         similarity = 0\n",
        "#         temp = [0]\n",
        "\n",
        "#         for j in s:    \n",
        "#           temp.append(sim[j][i])\n",
        "\n",
        "#         similarity = np.max(temp)\n",
        "#         mrr_temp = (lam1  * scores[i]) - (lam2 * similarity)\n",
        "        \n",
        "#         if mrr_temp > mrr_score:\n",
        "#             mrr_score = mrr_temp\n",
        "#             iid = i\n",
        "#     if iid == '':\n",
        "#         iid = i\n",
        "\n",
        "#     temp = R.copy()\n",
        "#     if iid in temp: temp.remove(iid)\n",
        "#     R = temp.copy()\n",
        "    \n",
        "#     s.append(iid)\n",
        "#     final_scores.append(mrr_score)\n",
        "#     x += 1   \n",
        "   \n",
        "# s\n",
        "# final_scores\n",
        "# s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VkEMfRvIhKV"
      },
      "source": [
        "from typing import Sequence\n",
        "def mmr(iids : Sequence[int], scores : Sequence[float], embs : np.ndarray, alpha : float) -> Sequence[int]:\n",
        "\n",
        "  assert len(iids) == len(scores)\n",
        "  assert len(iids) == embs.shape[0]\n",
        "  assert len(embs.size()) == 2\n",
        "\n",
        "  R = iids\n",
        "  lam1 = alpha\n",
        "  lam2 = 1 - lam1\n",
        "  s = []\n",
        "  final_scores =[]\n",
        "  # print(R)\n",
        "  # print(scores)\n",
        "  while len(R) > 0:\n",
        "    mrr_score = 0\n",
        "    iid = ''\n",
        "    for i in R:\n",
        "        similarity = 0\n",
        "        temp = [0]\n",
        "        for j in s:  \n",
        "          # print(i)\n",
        "          # first = BPRMF._net.item_embeddings.weight[iids[iids.index(i)]]\n",
        "          # second = BPRMF._net.item_embeddings.weight[iids[iids.index(j)]]\n",
        "          first = embs[iids.index(i)]\n",
        "          second = embs[iids.index(j)]\n",
        "          cosine_similarities = nn.functional.cosine_similarity(second ,first , axis=0)\n",
        "          temp.append(cosine_similarities)\n",
        "        similarity = np.max(temp)\n",
        "        # print(i)\n",
        "        mrr_temp = (lam1  * scores[iids.index(i)]) - (lam2 * similarity)\n",
        "        if mrr_temp > mrr_score:\n",
        "            mrr_score = mrr_temp\n",
        "            iid = i\n",
        "    if iid == '':\n",
        "        iid = i\n",
        "   \n",
        "    R_copy = R.copy()\n",
        "    if iid in R_copy: R_copy.remove(iid)\n",
        "    R = R_copy.copy()\n",
        "   \n",
        "    \n",
        "    s.append(iid)\n",
        "    final_scores.append(mrr_score)\n",
        "  \n",
        "  # print('reorderd',s)\n",
        "  rtr_iids = s\n",
        "  \n",
        "  #input your solution here returns a re-ordering of iids, such that the first ranked item is first in the list\n",
        "\n",
        "  return rtr_iids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34QWxFVTfrLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9700ba3-f629-43d1-8e58-602c01abd98d"
      },
      "source": [
        "def run_MMR_testcases(mmrfn):\n",
        "  example_embeddings1 = torch.tensor([[1.0,1.0],[1.0,1.0],[0,1.0],[0.1, 1.0]])\n",
        "  example_embeddings2 = torch.tensor([[1.0,1.0],[1.0,1.0],[0.02,1.0],[0.01,1.0]])\n",
        "  print(\"Testcase 0 : %s\" % mmrfn([1,2,3,4], [0.5, 0.5, 0.5, 0.5],  example_embeddings1, 0.5)[0] )\n",
        "  print(\"Testcase 1 : %s\" % mmrfn([1,2,3,4], [0.5, 0.5, 0.5, 0.5],  example_embeddings1, 0.5)[1] )\n",
        "  print(\"Testcase 2 : %s\" % mmrfn([1,2,3,4], [4, 3, 2, 1],  example_embeddings1, 1)[1] )\n",
        "  print(\"Testcase 3 : %s\" % mmrfn([1,2,3,4], [0.99, 0.98, 0.97, 0.001],  example_embeddings2, 0.001)[1] )\n",
        "  print(\"Testcase 4 : %s\" % mmrfn([1,2,3,4], [0.99, 0.98, 0.97, 0.001],  example_embeddings2, 0.5)[1] )\n",
        "\n",
        "run_MMR_testcases(mmr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testcase 0 : 1\n",
            "Testcase 1 : 4\n",
            "Testcase 2 : 2\n",
            "Testcase 3 : 4\n",
            "Testcase 4 : 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vc_YpPe6Fof"
      },
      "source": [
        "# l = np.array([0,2,4,5,6])\n",
        "# # l.indexOf(2)\n",
        "# a,b,c = get_top_K(BPRMF, 179, 10)\n",
        "# type(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT_sgiXg3PCy"
      },
      "source": [
        "# # a,b,c = get_top_K(BPRMF, 179, 10)\n",
        "# print('c',c[2],a[2])\n",
        "# a1 = mmr( a.tolist(),b,c, 0.5)\n",
        "# # print(a1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfm9mCWZmBPQ"
      },
      "source": [
        "Now we can analyse the impact of our MMR implementation. Let's consider again uid 179 (user u1331). \n",
        "\n",
        "Apply MMR on the top 10 results obtained from the BPRMF model using `get_top_K()`, with an alpha value of 0.5. The following code should help:\n",
        "```python\n",
        "mmr( *get_top_K(bprmodel, 179, 10), 0.5)\n",
        "```\n",
        "\n",
        "Finally, anayse the returned books. Calculate the ILD (with `k=5`), and examine the authors and titles (using `getAuthorTitle()`). \n",
        "\n",
        "Now answer the questions in Task 6 of the Moodle quiz.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkgLz33zAb5_",
        "outputId": "8705b1fc-fca0-4fa3-9587-18fc03d0b0e0"
      },
      "source": [
        "iids,b,c = get_top_K(BPRMF, 179, 10)\n",
        "print(iids)\n",
        "ILD = measure_ild(iids)\n",
        "print(round(ILD,4))\n",
        "iids_reranked = a1 = mmr( iids.tolist(), b, c, 0.5)\n",
        "print(iids_reranked)\n",
        "for iid in iids_reranked:\n",
        "  print(getAuthorTitle(iid))\n",
        "ILD = measure_ild(iids_reranked[0:5])\n",
        "print(round(ILD,4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 89 391  92 906  88  86  62  91   9 934]\n",
            "0.0619\n",
            "[89, 9, 88, 391, 906, 62, 934, 86, 92, 91]\n",
            "John Grisham / The Partner\n",
            "J.K. Rowling, Mary GrandPrÃ© / Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\n",
            "John Grisham / The Street Lawyer\n",
            "John Grisham / The Pelican Brief\n",
            "John Grisham / The Brethren\n",
            "John Grisham / The Rainmaker\n",
            "John Grisham / The Runaway Jury\n",
            "John Grisham / The Broker\n",
            "John Grisham / The Client\n",
            "John Grisham / The King of Torts\n",
            "0.5566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49QoGVphnegD"
      },
      "source": [
        "# Task 7\n",
        "\n",
        "This task is not a practical task - instead there are questions that tests your understanding of some related content of the course in the quiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8N-fEdkMKLD"
      },
      "source": [
        "# End of Exercise\n",
        "\n",
        "As part of your submission, you should complete the Exercise 3 quiz on Moodle.\n",
        "You will need to upload your notebook, complete with the **results** of executing the code."
      ]
    }
  ]
}